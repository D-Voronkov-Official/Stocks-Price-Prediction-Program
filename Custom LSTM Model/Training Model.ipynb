{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0244e1fe-230b-4b00-89ee-cf50a42ea436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.utils.data as data\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import gc\n",
    "import os\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e309a2f-d8f5-45be-8a79-c2b23bc2f281",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Model.ipynb\n",
    "%run \"../Machine Learning Data Gathering/Scripts/Moving Average + Bollinger bands.ipynb\"\n",
    "%run StocksDataSet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a57703c7-b387-4795-bc1d-e80d2b59dc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb9256a3-e366-4797-910d-e0dab456b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256\"\n",
    "# del os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "620ee46a-0b0b-4d55-bd6a-a32736d254d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a498230d-92fd-4c50-b640-599d33374e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(input, period_out):\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(input) - period_out - 1):\n",
    "        X.append(input[i: (i+period_out)])\n",
    "        y.append(input[i + period_out])\n",
    "    return torch.Tensor(np.array(X)), torch.Tensor(np.array(y))\n",
    "\n",
    "def test_func(input_data, output_data, steps_for_input, steps_for_output):\n",
    "    X,y = [], []\n",
    "    \n",
    "    for i in range(len(input_data)):\n",
    "        end_x = i + steps_for_input\n",
    "       # print(end_x)\n",
    "        output_x = end_x + steps_for_output - 1\n",
    "        if output_x > len(input_data): \n",
    "            break\n",
    "        #print(output_data[end_x - 1 : output_x])\n",
    "        seq_x, seq_y = input_data[i : end_x], output_data[end_x - 1 : output_x]\n",
    "        X.append(seq_x), y.append(seq_y)\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581a0b0c-f791-4236-9684-79becb707ffd",
   "metadata": {},
   "source": [
    "# Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc42d833-b7ad-4790-9dea-e1426cf91df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_data(X_full, y_full, split_percentage = 0.80, test_scaler = StandardScaler(), mm_scaler = MinMaxScaler()):\n",
    "#     total_data = len(X_full)\n",
    "#     test_split = round(split_percentage * total_data)\n",
    "#     X_train = torch.Tensor(X_full[:test_split])\n",
    "#     X_test = torch.Tensor(X_full[test_split:])\n",
    "\n",
    "#     y_train = torch.Tensor(y_full[:test_split])\n",
    "#     y_test = torch.Tensor(y_full[test_split:])\n",
    "\n",
    "#     X_train = torch.Tensor(test_scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape))\n",
    "#     X_test = torch.Tensor(test_scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape))\n",
    "    \n",
    "#     y_train = torch.Tensor(mm_scaler.fit_transform(y_train.reshape(-1, y_train.shape[-1])).reshape(y_train.shape))\n",
    "#     y_test = torch.Tensor(mm_scaler.transform(y_test.reshape(-1, y_test.shape[-1])).reshape(y_test.shape))\n",
    "\n",
    "#     return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e702fa1-eadb-451a-a06b-dfb7a8e5fcb4",
   "metadata": {},
   "source": [
    "# Predicting the stock price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "acab2588-7f51-4c4f-8623-c9c408c8f0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_stock_price(df, model):\n",
    "    model = model.to(device)\n",
    "    preds_test = df[-90:]\n",
    "    close_price = np.array(preds_test[\"Close\"])\n",
    "    close_price = close_price[-30:]\n",
    "    preds_test = preds_test.drop(\"Close\", axis = 1)\n",
    "    tens_preds = np.array(preds_test)\n",
    "    torch_test = torch.Tensor(tens_preds).unsqueeze(0)\n",
    "    test_scaler = StandardScaler()\n",
    "    mm_scaler = MinMaxScaler()\n",
    "    X_final = torch.Tensor(test_scaler.fit_transform(torch_test.reshape(-1, torch_test.shape[-1])).reshape(torch_test.shape))\n",
    "    y_final = torch.Tensor(mm_scaler.fit_transform(close_price.reshape(-1, close_price.shape[-1])).reshape(close_price.shape))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(X_final)\n",
    "    \n",
    "    preds = pred.detach().cpu()\n",
    "    print(preds)\n",
    "    \n",
    "    preds = mm_scaler.inverse_transform(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99119751-ca48-4bda-822c-aab8c1ccaf72",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa144993-e535-4395-baf3-8be7df15b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(ds, \n",
    "                input_size = 16, \n",
    "                hidden_size = 256, \n",
    "                num_layers = 1, \n",
    "                num_classes = 16, \n",
    "                batch_first = True, \n",
    "                epochs_number = 5000, \n",
    "                device = device, \n",
    "                learning_rate = 0.001):\n",
    "    \n",
    "    model = StocksPredictionModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes = num_classes, batch_first=True)\n",
    "    custom_ds = ds\n",
    "    \n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=learning_rate)\n",
    "    epochs = epochs_number\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for X_train, X_test, y_train, y_test in custom_ds:\n",
    "            X_train = torch.Tensor(X_train).squeeze(0)\n",
    "            model.train()\n",
    "            model = model.to(device)\n",
    "            X_train.to(device)\n",
    "       \n",
    "            y_pred = model(X_train)\n",
    "         \n",
    "          \n",
    "            optimizer.zero_grad()\n",
    "            y_pred = y_pred.to(device)\n",
    "            y_train = y_train.to(device)\n",
    "            loss = loss_fn(y_pred, y_train)\n",
    "            loss = loss.to(device)\n",
    "            # torch.cuda.empty_cache()\n",
    "            # gc.collect()\n",
    "            loss.backward()\n",
    "        \n",
    "            optimizer.step()\n",
    "        \n",
    "            model.eval()\n",
    "            \n",
    "        \n",
    "            with torch.inference_mode():\n",
    "            \n",
    "                test_pred = model(X_test)\n",
    "                test_pred.to(device)\n",
    "                y_test = y_test.to(device)\n",
    "                test_loss = loss_fn(test_pred, y_test)\n",
    "                if epoch == epochs - 1 or epoch % 100 == 0:\n",
    "                    print(f\"Epoch number: {epoch}\")\n",
    "                    print(f\"Test Loss is: {test_loss}\")\n",
    "                    print(f\"Train Loss is: {loss}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad73d0d-30ec-49d0-ab2f-bb651fe6ca26",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38909c9e-cd8d-49e1-bbf9-c419675f200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_to_save, model_name = \"01_stocks_predictions_model.pth\"):\n",
    "    MODEL_NAME = model_name\n",
    "    torch.save(model_to_save.state_dict(), f\"./Models/{MODEL_NAME}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d318115d-551b-4119-b9c8-58520202cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_with_kwargs(model, name = \"01_stocks_predictions_model_with_kwargs\"):\n",
    "    torch.save([model.kwargs, model.state_dict()], f\"./Models/{name}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3307a6-ed97-4296-9b88-d35e59b2626a",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8718271-2cc1-440b-9f90-fa4a39684c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name = \"01_stocks_predictions_model\"):\n",
    "    loaded_model = StocksPredictionModel(input_size=16, hidden_size=256, num_layers=1, num_classes = 30, batch_first=True)\n",
    "    loaded_model.load_state_dict(torch.load(f\"./Models/{model_name}.pth\"))\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7b9157d-f51b-4344-b220-3b52b47106c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_with_kwargs(model_name):\n",
    "    kwargs, state_dict = torch.load(f\"./Models/{model_name}.pth\")\n",
    "    model = StocksPredictionModel(**kwargs)\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4459c1f6-5c50-4a05-a8b4-10840622a626",
   "metadata": {},
   "source": [
    "# Transform data into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b396eb58-24c0-465e-a377-f5c911dd6b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_mlmodel(input_data, output_data, steps_for_input, steps_for_output):\n",
    "    X,y = [], []\n",
    "    print(input_data.shape)\n",
    "    test = steps_for_input + steps_for_output\n",
    "    print(len(input_data.index))\n",
    "   \n",
    "    rows_amount = len(input_data.index)\n",
    "    if rows_amount % test != 0:\n",
    "        input_data = input_data.iloc[rows_amount % test:]\n",
    "        output_data = output_data.iloc[rows_amount % test:]\n",
    "        rows_amount = len(input_data.index)\n",
    "    print(input_data.shape)\n",
    "    print(rows_amount)\n",
    "\n",
    "    for i in range(0, rows_amount, test):\n",
    "        X.append(input_data[i: i + steps_for_input])\n",
    "        y.append(output_data[i + steps_for_input : (i + steps_for_input) + steps_for_output])\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0f787f3-03ad-4ddb-bd86-abee01c50cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 0\n",
      "Test Loss is: 0.03750849515199661\n",
      "Train Loss is: 0.06132309138774872\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.10096871107816696\n",
      "Train Loss is: 0.20337563753128052\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.02492496371269226\n",
      "Train Loss is: 0.2506926357746124\n",
      "Epoch number: 0\n",
      "Test Loss is: 1.282423496246338\n",
      "Train Loss is: 0.19228757917881012\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.3497951030731201\n",
      "Train Loss is: 0.2707898020744324\n",
      "Epoch number: 0\n",
      "Test Loss is: 2.2347185611724854\n",
      "Train Loss is: 0.19641298055648804\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.23408961296081543\n",
      "Train Loss is: 0.21431131660938263\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.07669933885335922\n",
      "Train Loss is: 0.21253609657287598\n",
      "Epoch number: 0\n",
      "Test Loss is: 1.5979132652282715\n",
      "Train Loss is: 0.11829227954149246\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.33617883920669556\n",
      "Train Loss is: 0.2697064280509949\n",
      "Epoch number: 0\n",
      "Test Loss is: 1.3618184328079224\n",
      "Train Loss is: 0.18678365647792816\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.010257234796881676\n",
      "Train Loss is: 0.16803330183029175\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.4502483308315277\n",
      "Train Loss is: 0.473390132188797\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.5701830387115479\n",
      "Train Loss is: 0.11774981766939163\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.5339218974113464\n",
      "Train Loss is: 0.16352586448192596\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.5652448534965515\n",
      "Train Loss is: 0.19335655868053436\n",
      "Epoch number: 0\n",
      "Test Loss is: 53.41917419433594\n",
      "Train Loss is: 0.11005765199661255\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.031169410794973373\n",
      "Train Loss is: 0.09131380915641785\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.21006563305854797\n",
      "Train Loss is: 0.14540210366249084\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.6155967116355896\n",
      "Train Loss is: 0.10504710674285889\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.04336538538336754\n",
      "Train Loss is: 0.045478999614715576\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.12391278892755508\n",
      "Train Loss is: 0.11096996068954468\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.14886900782585144\n",
      "Train Loss is: 0.04822412505745888\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.581354022026062\n",
      "Train Loss is: 0.1240297183394432\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.29206758737564087\n",
      "Train Loss is: 0.10490762442350388\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.02191951312124729\n",
      "Train Loss is: 0.04513169825077057\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.6039241552352905\n",
      "Train Loss is: 0.04192941635847092\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.3749571740627289\n",
      "Train Loss is: 0.044190552085638046\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.020627044141292572\n",
      "Train Loss is: 0.03381311893463135\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.05667514726519585\n",
      "Train Loss is: 0.052381567656993866\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.03712460398674011\n",
      "Train Loss is: 0.03087439574301243\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.24351376295089722\n",
      "Train Loss is: 0.10913553088903427\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.07073739171028137\n",
      "Train Loss is: 0.04185175523161888\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.012497399933636189\n",
      "Train Loss is: 0.020473003387451172\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.03348873555660248\n",
      "Train Loss is: 0.07961862534284592\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.009810848161578178\n",
      "Train Loss is: 0.1032448336482048\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.05448834225535393\n",
      "Train Loss is: 0.17200200259685516\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.018131013959646225\n",
      "Train Loss is: 0.05309361591935158\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.11353175342082977\n",
      "Train Loss is: 0.10377391427755356\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.012817478738725185\n",
      "Train Loss is: 0.1190626323223114\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.01272827573120594\n",
      "Train Loss is: 0.1348632276058197\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.01913090981543064\n",
      "Train Loss is: 0.042501822113990784\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.011697321198880672\n",
      "Train Loss is: 0.023753676563501358\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.008299224078655243\n",
      "Train Loss is: 0.022736553102731705\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.022007450461387634\n",
      "Train Loss is: 0.04145609959959984\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.6316405534744263\n",
      "Train Loss is: 0.05271022394299507\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.5220674872398376\n",
      "Train Loss is: 0.06214234605431557\n",
      "Epoch number: 0\n",
      "Test Loss is: 1.4808506965637207\n",
      "Train Loss is: 0.051205772906541824\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.04365040734410286\n",
      "Train Loss is: 0.04927825182676315\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.08746185898780823\n",
      "Train Loss is: 0.11067993938922882\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.7829405665397644\n",
      "Train Loss is: 0.1683005392551422\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.014366785064339638\n",
      "Train Loss is: 0.018508106470108032\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.007987750694155693\n",
      "Train Loss is: 0.07594490796327591\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.005015026777982712\n",
      "Train Loss is: 0.0299115888774395\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.018467063084244728\n",
      "Train Loss is: 0.06343104690313339\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.01968567632138729\n",
      "Train Loss is: 0.19244377315044403\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.20350684225559235\n",
      "Train Loss is: 0.10710947960615158\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.03913062810897827\n",
      "Train Loss is: 0.08153625577688217\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.23608948290348053\n",
      "Train Loss is: 0.019731076434254646\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.04906744137406349\n",
      "Train Loss is: 0.02014186978340149\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.025529995560646057\n",
      "Train Loss is: 0.024391543120145798\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.1353391408920288\n",
      "Train Loss is: 0.04837198182940483\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.06944380700588226\n",
      "Train Loss is: 0.08377227932214737\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.05866747722029686\n",
      "Train Loss is: 0.05302519351243973\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.05640073120594025\n",
      "Train Loss is: 0.10034701228141785\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.3141096830368042\n",
      "Train Loss is: 0.03822319209575653\n",
      "Epoch number: 0\n",
      "Test Loss is: 2.896918296813965\n",
      "Train Loss is: 0.025786802172660828\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.011103350669145584\n",
      "Train Loss is: 0.027098124846816063\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.14371119439601898\n",
      "Train Loss is: 0.009530364535748959\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.5475344061851501\n",
      "Train Loss is: 0.010699613951146603\n",
      "Epoch number: 0\n",
      "Test Loss is: 0.5145458579063416\n",
      "Train Loss is: 0.012595415115356445\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.00872189924120903\n",
      "Train Loss is: 0.0034824556205421686\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.0032462929375469685\n",
      "Train Loss is: 0.007135315332561731\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.001311127794906497\n",
      "Train Loss is: 0.004645568318665028\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.3935838043689728\n",
      "Train Loss is: 0.004432785790413618\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.01048462837934494\n",
      "Train Loss is: 0.010926785878837109\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.8631699085235596\n",
      "Train Loss is: 0.007802971173077822\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.009882049635052681\n",
      "Train Loss is: 0.0113849937915802\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.024281200021505356\n",
      "Train Loss is: 0.006488883402198553\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.8889168500900269\n",
      "Train Loss is: 0.0034429680090397596\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.007245014421641827\n",
      "Train Loss is: 0.010712172836065292\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.7834205627441406\n",
      "Train Loss is: 0.008985023014247417\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.001213102019391954\n",
      "Train Loss is: 0.006809520535171032\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.23357723653316498\n",
      "Train Loss is: 0.019883427768945694\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.15165449678897858\n",
      "Train Loss is: 0.005409122910350561\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.1699492484331131\n",
      "Train Loss is: 0.013101336546242237\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.027200615033507347\n",
      "Train Loss is: 0.0047889333218336105\n",
      "Epoch number: 100\n",
      "Test Loss is: 56.58723449707031\n",
      "Train Loss is: 0.019395602867007256\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.004283000249415636\n",
      "Train Loss is: 0.004931765142828226\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.0124337924644351\n",
      "Train Loss is: 0.007741465698927641\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.13949687778949738\n",
      "Train Loss is: 0.005739751271903515\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.048359375447034836\n",
      "Train Loss is: 0.01605161838233471\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.022234993055462837\n",
      "Train Loss is: 0.008449830114841461\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.04524240642786026\n",
      "Train Loss is: 0.004814785439521074\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.3845571279525757\n",
      "Train Loss is: 0.0035297488793730736\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.03476738557219505\n",
      "Train Loss is: 0.01368030533194542\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.004504531621932983\n",
      "Train Loss is: 0.01057997066527605\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.41951438784599304\n",
      "Train Loss is: 0.00846576876938343\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.09197141975164413\n",
      "Train Loss is: 0.007837871089577675\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.002188112586736679\n",
      "Train Loss is: 0.003469794988632202\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.035381805151700974\n",
      "Train Loss is: 0.006576778832823038\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.036609549075365067\n",
      "Train Loss is: 0.0037560274358838797\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.015888839960098267\n",
      "Train Loss is: 0.006863775197416544\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.016838518902659416\n",
      "Train Loss is: 0.005383969284594059\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.00021498341811820865\n",
      "Train Loss is: 0.002171617466956377\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.00973705668002367\n",
      "Train Loss is: 0.0076303607784211636\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.0006895624683238566\n",
      "Train Loss is: 0.011930223554372787\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.043644919991493225\n",
      "Train Loss is: 0.01157487090677023\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.02164578251540661\n",
      "Train Loss is: 0.020214062184095383\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.08575937151908875\n",
      "Train Loss is: 0.016878873109817505\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.002477798145264387\n",
      "Train Loss is: 0.006052238866686821\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.00047167998855002224\n",
      "Train Loss is: 0.004371017217636108\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.0008448749431408942\n",
      "Train Loss is: 0.005028670188039541\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.004422314930707216\n",
      "Train Loss is: 0.007114885374903679\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.0015679775970056653\n",
      "Train Loss is: 0.004870660603046417\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.005354265682399273\n",
      "Train Loss is: 0.0023934582713991404\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.1812666803598404\n",
      "Train Loss is: 0.00997971836477518\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.2067626714706421\n",
      "Train Loss is: 0.004021720960736275\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.8926436305046082\n",
      "Train Loss is: 0.006058800499886274\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.0064356354996562\n",
      "Train Loss is: 0.004314024932682514\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.0091820377856493\n",
      "Train Loss is: 0.008326581679284573\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.17503857612609863\n",
      "Train Loss is: 0.012609833851456642\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.02695246785879135\n",
      "Train Loss is: 0.013743649236857891\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.003471614792943001\n",
      "Train Loss is: 0.004265078343451023\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.0010294158710166812\n",
      "Train Loss is: 0.003936238121241331\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.009788814932107925\n",
      "Train Loss is: 0.003150312462821603\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.013475150801241398\n",
      "Train Loss is: 0.033309608697891235\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.020991122350096703\n",
      "Train Loss is: 0.01228244323283434\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.015865061432123184\n",
      "Train Loss is: 0.013265428133308887\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.2674636244773865\n",
      "Train Loss is: 0.004866973962634802\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.0218181349337101\n",
      "Train Loss is: 0.009242530912160873\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.010199418291449547\n",
      "Train Loss is: 0.008733713068068027\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.031533315777778625\n",
      "Train Loss is: 0.012588029727339745\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.004732166882604361\n",
      "Train Loss is: 0.007105325814336538\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.021831993013620377\n",
      "Train Loss is: 0.01229572668671608\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.060947123914957047\n",
      "Train Loss is: 0.001367740798741579\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.6588398814201355\n",
      "Train Loss is: 0.009054603986442089\n",
      "Epoch number: 100\n",
      "Test Loss is: 3.5101065635681152\n",
      "Train Loss is: 0.0026566239539533854\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.007054678630083799\n",
      "Train Loss is: 0.004090287256985903\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.08513420820236206\n",
      "Train Loss is: 0.004125732928514481\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.4759442210197449\n",
      "Train Loss is: 0.007325071841478348\n",
      "Epoch number: 100\n",
      "Test Loss is: 0.5050032734870911\n",
      "Train Loss is: 0.004711020737886429\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.0039756326004862785\n",
      "Train Loss is: 0.0020992939826101065\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.0027232335414737463\n",
      "Train Loss is: 0.0033523403108119965\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.0017224372131749988\n",
      "Train Loss is: 0.005382220726460218\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.3259287476539612\n",
      "Train Loss is: 0.003374519757926464\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.00741736963391304\n",
      "Train Loss is: 0.004391185473650694\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.5665045380592346\n",
      "Train Loss is: 0.004468242637813091\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.010351880453526974\n",
      "Train Loss is: 0.004812856204807758\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.024911904707551003\n",
      "Train Loss is: 0.0034012519754469395\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.5078026652336121\n",
      "Train Loss is: 0.006308747921139002\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.03890947997570038\n",
      "Train Loss is: 0.00796008575707674\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.402151495218277\n",
      "Train Loss is: 0.0027149345260113478\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.003004792844876647\n",
      "Train Loss is: 0.0030372266191989183\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.2303319126367569\n",
      "Train Loss is: 0.005396698601543903\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.044719260185956955\n",
      "Train Loss is: 0.0029116144869476557\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.07999500632286072\n",
      "Train Loss is: 0.004788476508110762\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.014330959878861904\n",
      "Train Loss is: 0.003046254627406597\n",
      "Epoch number: 200\n",
      "Test Loss is: 56.66022872924805\n",
      "Train Loss is: 0.012847667559981346\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.005835894029587507\n",
      "Train Loss is: 0.00256568961776793\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.013520998880267143\n",
      "Train Loss is: 0.004320278763771057\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.04744056984782219\n",
      "Train Loss is: 0.006303959991782904\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.06306356936693192\n",
      "Train Loss is: 0.006140357349067926\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.025100022554397583\n",
      "Train Loss is: 0.004129873123019934\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.012187841348350048\n",
      "Train Loss is: 0.004866062197834253\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.3974456489086151\n",
      "Train Loss is: 0.002653916832059622\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.03785504028201103\n",
      "Train Loss is: 0.00429390836507082\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.008330845274031162\n",
      "Train Loss is: 0.004752590786665678\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.33485791087150574\n",
      "Train Loss is: 0.0033793444745242596\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.12544961273670197\n",
      "Train Loss is: 0.001568000647239387\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.002711614826694131\n",
      "Train Loss is: 0.003155931830406189\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.029045194387435913\n",
      "Train Loss is: 0.00281233387067914\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.10175629705190659\n",
      "Train Loss is: 0.003379181260243058\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.043476976454257965\n",
      "Train Loss is: 0.006711711175739765\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.020099161192774773\n",
      "Train Loss is: 0.005560775753110647\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.00023528172459919006\n",
      "Train Loss is: 0.002245931653305888\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.015342372469604015\n",
      "Train Loss is: 0.005053578410297632\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.004020176827907562\n",
      "Train Loss is: 0.005870729219168425\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.06294652819633484\n",
      "Train Loss is: 0.0036016551312059164\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.016379324719309807\n",
      "Train Loss is: 0.007097659632563591\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.06949632614850998\n",
      "Train Loss is: 0.005252236034721136\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.003053020453080535\n",
      "Train Loss is: 0.003302251920104027\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.0006194914458319545\n",
      "Train Loss is: 0.004161051008850336\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.000849183474201709\n",
      "Train Loss is: 0.003962703049182892\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.0027558335568755865\n",
      "Train Loss is: 0.004725935403257608\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.0005779547500424087\n",
      "Train Loss is: 0.004881434608250856\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.012898428365588188\n",
      "Train Loss is: 0.0013814797857776284\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.2008604109287262\n",
      "Train Loss is: 0.004246838390827179\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.22156265377998352\n",
      "Train Loss is: 0.0019649569876492023\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.9071173071861267\n",
      "Train Loss is: 0.004387922119349241\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.010068519972264767\n",
      "Train Loss is: 0.002863402711227536\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.008591360412538052\n",
      "Train Loss is: 0.0032627403270453215\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.21392257511615753\n",
      "Train Loss is: 0.0056098755449056625\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.014753879979252815\n",
      "Train Loss is: 0.0035434048622846603\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.010487310588359833\n",
      "Train Loss is: 0.0024909113999456167\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.0007857239688746631\n",
      "Train Loss is: 0.006675637327134609\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.013297811150550842\n",
      "Train Loss is: 0.002490806393325329\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.014751601032912731\n",
      "Train Loss is: 0.007242077030241489\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.022490892559289932\n",
      "Train Loss is: 0.003984492737799883\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.025867994874715805\n",
      "Train Loss is: 0.002870481228455901\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.2091328203678131\n",
      "Train Loss is: 0.00343984249047935\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.030580000951886177\n",
      "Train Loss is: 0.0033876148518174887\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.014937332831323147\n",
      "Train Loss is: 0.005288577638566494\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.05326803773641586\n",
      "Train Loss is: 0.008513481356203556\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.005751506425440311\n",
      "Train Loss is: 0.005958042107522488\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.013483266346156597\n",
      "Train Loss is: 0.004507896956056356\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.03730529174208641\n",
      "Train Loss is: 0.004223525989800692\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.599267840385437\n",
      "Train Loss is: 0.002447368111461401\n",
      "Epoch number: 200\n",
      "Test Loss is: 3.514434576034546\n",
      "Train Loss is: 0.001873872592113912\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.0037645839620381594\n",
      "Train Loss is: 0.003578255185857415\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.08347760885953903\n",
      "Train Loss is: 0.0015755514614284039\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.44482240080833435\n",
      "Train Loss is: 0.0021139204036444426\n",
      "Epoch number: 200\n",
      "Test Loss is: 0.4737344980239868\n",
      "Train Loss is: 0.0038476428017020226\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.009505651891231537\n",
      "Train Loss is: 0.0009156893938779831\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.0051673040725290775\n",
      "Train Loss is: 0.0013338917633518577\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.004794959910213947\n",
      "Train Loss is: 0.0015120662283152342\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.2504672706127167\n",
      "Train Loss is: 0.001292585046030581\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.015329010784626007\n",
      "Train Loss is: 0.0018966810312122107\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.5857395529747009\n",
      "Train Loss is: 0.0023901730310171843\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.01219464186578989\n",
      "Train Loss is: 0.0025207565631717443\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.011062316596508026\n",
      "Train Loss is: 0.002007894916459918\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.6406236290931702\n",
      "Train Loss is: 0.0018243364756926894\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.014618650078773499\n",
      "Train Loss is: 0.002172259846702218\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.6034461259841919\n",
      "Train Loss is: 0.0013524855021387339\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.0021220012567937374\n",
      "Train Loss is: 0.0018504767213016748\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.28878462314605713\n",
      "Train Loss is: 0.0009100186871364713\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.1150025799870491\n",
      "Train Loss is: 0.0012131835101172328\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.1426369994878769\n",
      "Train Loss is: 0.002064111642539501\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.01844310201704502\n",
      "Train Loss is: 0.0023120231926441193\n",
      "Epoch number: 300\n",
      "Test Loss is: 55.46479415893555\n",
      "Train Loss is: 0.002418170217424631\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.0032983161509037018\n",
      "Train Loss is: 0.0012733456678688526\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.022962981835007668\n",
      "Train Loss is: 0.002297752071171999\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.12711083889007568\n",
      "Train Loss is: 0.002579721389338374\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.028225328773260117\n",
      "Train Loss is: 0.0018403006251901388\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.030965421348810196\n",
      "Train Loss is: 0.002256708685308695\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.017090411856770515\n",
      "Train Loss is: 0.0022480871994048357\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.35060086846351624\n",
      "Train Loss is: 0.0018506828928366303\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.04157860949635506\n",
      "Train Loss is: 0.001998594729229808\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.0022144813556224108\n",
      "Train Loss is: 0.0018982874462381005\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.37958744168281555\n",
      "Train Loss is: 0.0006222390220500529\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.08288568258285522\n",
      "Train Loss is: 0.0009104099590331316\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.0003167557006236166\n",
      "Train Loss is: 0.0012239847565069795\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.032471030950546265\n",
      "Train Loss is: 0.0017605688190087676\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.09828927367925644\n",
      "Train Loss is: 0.0010703307343646884\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.02076454646885395\n",
      "Train Loss is: 0.0015425306046381593\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.014727798290550709\n",
      "Train Loss is: 0.0015139232855290174\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.0007007585372775793\n",
      "Train Loss is: 0.0006883788737468421\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.008071145974099636\n",
      "Train Loss is: 0.0014786286046728492\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.004566465970128775\n",
      "Train Loss is: 0.0022330516949295998\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.04262422397732735\n",
      "Train Loss is: 0.001994919730350375\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.017219576984643936\n",
      "Train Loss is: 0.0031071046832948923\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.0587385855615139\n",
      "Train Loss is: 0.002708886517211795\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.005384096410125494\n",
      "Train Loss is: 0.001447938964702189\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.0007774647092446685\n",
      "Train Loss is: 0.0014646223280578852\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.002592818345874548\n",
      "Train Loss is: 0.0015384614234790206\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.003039731876924634\n",
      "Train Loss is: 0.0015911045484244823\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.000843416026327759\n",
      "Train Loss is: 0.0012301965616643429\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.008448072709143162\n",
      "Train Loss is: 0.000926255714148283\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.17276659607887268\n",
      "Train Loss is: 0.001614605775102973\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.14611199498176575\n",
      "Train Loss is: 0.001204754109494388\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.789130449295044\n",
      "Train Loss is: 0.0013974226312711835\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.010330801829695702\n",
      "Train Loss is: 0.0015450631035491824\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.009008712135255337\n",
      "Train Loss is: 0.0018499426078051329\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.18752741813659668\n",
      "Train Loss is: 0.0029702233150601387\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.01639418490231037\n",
      "Train Loss is: 0.0010496532777324319\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.00504892086610198\n",
      "Train Loss is: 0.001205797423608601\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.001289809588342905\n",
      "Train Loss is: 0.0011714936699718237\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.013030651025474072\n",
      "Train Loss is: 0.0010773524409160018\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.020798837766051292\n",
      "Train Loss is: 0.0019523600349202752\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.037398748099803925\n",
      "Train Loss is: 0.0015062553575262427\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.024234680458903313\n",
      "Train Loss is: 0.00173617759719491\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.3714733421802521\n",
      "Train Loss is: 0.0014919721288606524\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.014506938867270947\n",
      "Train Loss is: 0.0014560760464519262\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.009984581731259823\n",
      "Train Loss is: 0.002337703015655279\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.04502256214618683\n",
      "Train Loss is: 0.0038904522079974413\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.007556111551821232\n",
      "Train Loss is: 0.0020501133985817432\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.011977797374129295\n",
      "Train Loss is: 0.0022179551888257265\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.061929505318403244\n",
      "Train Loss is: 0.0006352934869937599\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.7220427989959717\n",
      "Train Loss is: 0.0012947130016982555\n",
      "Epoch number: 300\n",
      "Test Loss is: 3.5842676162719727\n",
      "Train Loss is: 0.0010509775020182133\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.005046181846410036\n",
      "Train Loss is: 0.0008163691964000463\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.06566908210515976\n",
      "Train Loss is: 0.0005646407371386886\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.3884226679801941\n",
      "Train Loss is: 0.0007003225618973374\n",
      "Epoch number: 300\n",
      "Test Loss is: 0.46510598063468933\n",
      "Train Loss is: 0.0011932090856134892\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.0086094094440341\n",
      "Train Loss is: 0.00064602040220052\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.00206838920712471\n",
      "Train Loss is: 0.000751389074139297\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.0065317582339048386\n",
      "Train Loss is: 0.001019149087369442\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.31102272868156433\n",
      "Train Loss is: 0.0007095988257788122\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.008351309224963188\n",
      "Train Loss is: 0.0012758028460666537\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.5714324116706848\n",
      "Train Loss is: 0.0012159395264461637\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.006139282137155533\n",
      "Train Loss is: 0.0016849617240950465\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.015525761991739273\n",
      "Train Loss is: 0.0009205109672620893\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.6845576763153076\n",
      "Train Loss is: 0.0005917784292250872\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.005897632334381342\n",
      "Train Loss is: 0.001362149603664875\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.5488589406013489\n",
      "Train Loss is: 0.0006932563846930861\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.0013862400082871318\n",
      "Train Loss is: 0.0012845193268731236\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.28336530923843384\n",
      "Train Loss is: 0.0007418814930133522\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.08735863864421844\n",
      "Train Loss is: 0.000776543456595391\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.10311026871204376\n",
      "Train Loss is: 0.0011935729999095201\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.018434304744005203\n",
      "Train Loss is: 0.00118326919618994\n",
      "Epoch number: 400\n",
      "Test Loss is: 55.82318878173828\n",
      "Train Loss is: 0.0021063259337097406\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.004684326238930225\n",
      "Train Loss is: 0.0007615997456014156\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.017190923914313316\n",
      "Train Loss is: 0.001045415410771966\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.10323571413755417\n",
      "Train Loss is: 0.0017994360532611609\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.036651790142059326\n",
      "Train Loss is: 0.0012426170287653804\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.026309791952371597\n",
      "Train Loss is: 0.001046119723469019\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.00853605754673481\n",
      "Train Loss is: 0.0016213631024584174\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.35640788078308105\n",
      "Train Loss is: 0.0012570994440466166\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.046531904488801956\n",
      "Train Loss is: 0.0015197611646726727\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.004813225939869881\n",
      "Train Loss is: 0.0014955798396840692\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.3288864195346832\n",
      "Train Loss is: 0.0005377074703574181\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.09621651470661163\n",
      "Train Loss is: 0.0005457481020130217\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.00019435783906374127\n",
      "Train Loss is: 0.0006040338193997741\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.040232934057712555\n",
      "Train Loss is: 0.00129264360293746\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.10947053134441376\n",
      "Train Loss is: 0.0005965181044302881\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.028193127363920212\n",
      "Train Loss is: 0.0009119935566559434\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.015613489784300327\n",
      "Train Loss is: 0.00103675143327564\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.00010790690430440009\n",
      "Train Loss is: 0.0003817493561655283\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.007593479007482529\n",
      "Train Loss is: 0.0011123575968667865\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.004063958767801523\n",
      "Train Loss is: 0.0015316253993660212\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.04221191629767418\n",
      "Train Loss is: 0.0015601112972944975\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.01708325743675232\n",
      "Train Loss is: 0.0025063329376280308\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.07156913727521896\n",
      "Train Loss is: 0.001883520744740963\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.002720752265304327\n",
      "Train Loss is: 0.0010108917485922575\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.0026567946188151836\n",
      "Train Loss is: 0.0009025830077007413\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.0018321833340451121\n",
      "Train Loss is: 0.0010086848633363843\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.003752936841920018\n",
      "Train Loss is: 0.0009530069073662162\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.0009221212822012603\n",
      "Train Loss is: 0.0006920072482898831\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.011798743158578873\n",
      "Train Loss is: 0.00024654134176671505\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.19143150746822357\n",
      "Train Loss is: 0.0016005432698875666\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.11749710142612457\n",
      "Train Loss is: 0.0005819326615892351\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.5844902396202087\n",
      "Train Loss is: 0.0005677299341186881\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.013017263263463974\n",
      "Train Loss is: 0.0008555219974368811\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.008407429791986942\n",
      "Train Loss is: 0.0013081743381917477\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.1427859216928482\n",
      "Train Loss is: 0.0021375531796365976\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.03863850235939026\n",
      "Train Loss is: 0.0008086302550509572\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.0047599985264241695\n",
      "Train Loss is: 0.0010645618895068765\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.0008657628204673529\n",
      "Train Loss is: 0.0010945184621959925\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.005263133905827999\n",
      "Train Loss is: 0.00087328179506585\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.013827887363731861\n",
      "Train Loss is: 0.0014483006671071053\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.04028317704796791\n",
      "Train Loss is: 0.0008171721128746867\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.02368699014186859\n",
      "Train Loss is: 0.0008169144857674837\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.3291482627391815\n",
      "Train Loss is: 0.0006716244970448315\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.015891695395112038\n",
      "Train Loss is: 0.0006376524688676\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.013170858845114708\n",
      "Train Loss is: 0.0012103237677365541\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.03172818943858147\n",
      "Train Loss is: 0.003147086361423135\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.007822716608643532\n",
      "Train Loss is: 0.0011853835312649608\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.01235218159854412\n",
      "Train Loss is: 0.0011389063438400626\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.04995127394795418\n",
      "Train Loss is: 0.000423485558712855\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.740037202835083\n",
      "Train Loss is: 0.0006838213885203004\n",
      "Epoch number: 400\n",
      "Test Loss is: 3.736886739730835\n",
      "Train Loss is: 0.00047629879554733634\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.0044865538366138935\n",
      "Train Loss is: 0.0006086590001359582\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.06546403467655182\n",
      "Train Loss is: 0.00044624984730035067\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.4362373352050781\n",
      "Train Loss is: 0.0005260260077193379\n",
      "Epoch number: 400\n",
      "Test Loss is: 0.5140987634658813\n",
      "Train Loss is: 0.000959138385951519\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.008280734531581402\n",
      "Train Loss is: 0.0004932953743264079\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.002378204371780157\n",
      "Train Loss is: 0.0006443884340114892\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.004590174648910761\n",
      "Train Loss is: 0.00100947346072644\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.3414176404476166\n",
      "Train Loss is: 0.000532648409716785\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.011855419725179672\n",
      "Train Loss is: 0.00126207759603858\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.7407979369163513\n",
      "Train Loss is: 0.0009934708941727877\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.00591735914349556\n",
      "Train Loss is: 0.001415463862940669\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.013799937441945076\n",
      "Train Loss is: 0.0008157133706845343\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.8540234565734863\n",
      "Train Loss is: 0.0005413570906966925\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.009413231164216995\n",
      "Train Loss is: 0.0013755642576143146\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.7550390362739563\n",
      "Train Loss is: 0.0005721993511542678\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.005526389926671982\n",
      "Train Loss is: 0.0011837446363642812\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.2867581844329834\n",
      "Train Loss is: 0.0007256570970639586\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.12223600596189499\n",
      "Train Loss is: 0.0005539264529943466\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.14086759090423584\n",
      "Train Loss is: 0.0010122585808858275\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.013103135861456394\n",
      "Train Loss is: 0.001087979064323008\n",
      "Epoch number: 500\n",
      "Test Loss is: 55.6509895324707\n",
      "Train Loss is: 0.004360948223620653\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.0038888566195964813\n",
      "Train Loss is: 0.0005600043805316091\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.017466053366661072\n",
      "Train Loss is: 0.0009228915441781282\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.1459536999464035\n",
      "Train Loss is: 0.0015284311957657337\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.03462086245417595\n",
      "Train Loss is: 0.0010071282740682364\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.026305045932531357\n",
      "Train Loss is: 0.000908869260456413\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.009406832046806812\n",
      "Train Loss is: 0.00137934775557369\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.3369591534137726\n",
      "Train Loss is: 0.0012547142105177045\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.05022430047392845\n",
      "Train Loss is: 0.0012960139429196715\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.0051072146743535995\n",
      "Train Loss is: 0.001334069762378931\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.5303974747657776\n",
      "Train Loss is: 0.0005305283702909946\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.14659422636032104\n",
      "Train Loss is: 0.0006409737979993224\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.0002923940191976726\n",
      "Train Loss is: 0.0005902819684706628\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.036320533603429794\n",
      "Train Loss is: 0.0012044422328472137\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.10268628597259521\n",
      "Train Loss is: 0.0005595608381554484\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.024457290768623352\n",
      "Train Loss is: 0.0008160551078617573\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.0191184114664793\n",
      "Train Loss is: 0.0008427287684753537\n",
      "Epoch number: 500\n",
      "Test Loss is: 3.753167038667016e-05\n",
      "Train Loss is: 0.0003466203052084893\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.00783967413008213\n",
      "Train Loss is: 0.0008150676148943603\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.002925661625340581\n",
      "Train Loss is: 0.0009899534052237868\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.047569166868925095\n",
      "Train Loss is: 0.0013813137775287032\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.016196930781006813\n",
      "Train Loss is: 0.0020097787491977215\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.06897203624248505\n",
      "Train Loss is: 0.0017377083422616124\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.0017405730905011296\n",
      "Train Loss is: 0.0009478744468651712\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.0014311025151982903\n",
      "Train Loss is: 0.0008329342817887664\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.0009780586697161198\n",
      "Train Loss is: 0.0009785182774066925\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.002572732511907816\n",
      "Train Loss is: 0.0010052534053102136\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.0009888469940051436\n",
      "Train Loss is: 0.0007499047205783427\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.00917073991149664\n",
      "Train Loss is: 0.00021008589828852564\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.18620522320270538\n",
      "Train Loss is: 0.0013978484785184264\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.1343807727098465\n",
      "Train Loss is: 0.000497876200824976\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.7952771186828613\n",
      "Train Loss is: 0.0005630336236208677\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.008645382709801197\n",
      "Train Loss is: 0.0006017813575454056\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.008277046494185925\n",
      "Train Loss is: 0.0010592290200293064\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.18454928696155548\n",
      "Train Loss is: 0.00161997159011662\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.016817400231957436\n",
      "Train Loss is: 0.00046879713772796094\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.006607550196349621\n",
      "Train Loss is: 0.0007683839066885412\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.0012356765801087022\n",
      "Train Loss is: 0.000742879812605679\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.005550698842853308\n",
      "Train Loss is: 0.0007211051997728646\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.015416916459798813\n",
      "Train Loss is: 0.0010572614846751094\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.050913576036691666\n",
      "Train Loss is: 0.0008255001157522202\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.02329808659851551\n",
      "Train Loss is: 0.0007383381016552448\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.365588515996933\n",
      "Train Loss is: 0.0006820621783845127\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.02056344598531723\n",
      "Train Loss is: 0.0006460708100348711\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.01003424171358347\n",
      "Train Loss is: 0.0011125876335427165\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.035229820758104324\n",
      "Train Loss is: 0.0027750961016863585\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.005482865497469902\n",
      "Train Loss is: 0.0010109097929671407\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.022943031042814255\n",
      "Train Loss is: 0.0008699970203451812\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.043471675366163254\n",
      "Train Loss is: 0.0002336019097128883\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.6973170042037964\n",
      "Train Loss is: 0.0006266636773943901\n",
      "Epoch number: 500\n",
      "Test Loss is: 3.5661368370056152\n",
      "Train Loss is: 0.0003895971749443561\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.0037599033676087856\n",
      "Train Loss is: 0.0005180087173357606\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.0651850551366806\n",
      "Train Loss is: 0.00037170195719227195\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.41504552960395813\n",
      "Train Loss is: 0.0003903136239387095\n",
      "Epoch number: 500\n",
      "Test Loss is: 0.49763405323028564\n",
      "Train Loss is: 0.0009525853092782199\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.016173994168639183\n",
      "Train Loss is: 0.0004938392667099833\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.002706314902752638\n",
      "Train Loss is: 0.0005239922902546823\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.006963874213397503\n",
      "Train Loss is: 0.0007370722596533597\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.29520711302757263\n",
      "Train Loss is: 0.0005074999062344432\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.0100654736161232\n",
      "Train Loss is: 0.0010743648745119572\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.6666702628135681\n",
      "Train Loss is: 0.0008700181497260928\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.00837000086903572\n",
      "Train Loss is: 0.0012617699103429914\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.01074253674596548\n",
      "Train Loss is: 0.0006374932127073407\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.7808211445808411\n",
      "Train Loss is: 0.00041057425551116467\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.009140104055404663\n",
      "Train Loss is: 0.0010586800053715706\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.656592845916748\n",
      "Train Loss is: 0.00046992048737592995\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.006168844178318977\n",
      "Train Loss is: 0.001021338626742363\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.2690478563308716\n",
      "Train Loss is: 0.0005534159718081355\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.11012967675924301\n",
      "Train Loss is: 0.0004485991958063096\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.13871900737285614\n",
      "Train Loss is: 0.0008202057797461748\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.020076362416148186\n",
      "Train Loss is: 0.0009713281178846955\n",
      "Epoch number: 600\n",
      "Test Loss is: 55.814170837402344\n",
      "Train Loss is: 0.0015547832008451223\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.004034111741930246\n",
      "Train Loss is: 0.00046232828754000366\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.022405344992876053\n",
      "Train Loss is: 0.0007363820332102478\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.14823782444000244\n",
      "Train Loss is: 0.0013693163637071848\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.029389245435595512\n",
      "Train Loss is: 0.0009184054797515273\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.025045204907655716\n",
      "Train Loss is: 0.0007911562570370734\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.011434104293584824\n",
      "Train Loss is: 0.0012527392245829105\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.3381698429584503\n",
      "Train Loss is: 0.0011147974291816354\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.04009866714477539\n",
      "Train Loss is: 0.0011676409048959613\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.007622298784554005\n",
      "Train Loss is: 0.0011377374175935984\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.3748236298561096\n",
      "Train Loss is: 0.0006024299655109644\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.12151578068733215\n",
      "Train Loss is: 0.0004108544671908021\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.0005397977656684816\n",
      "Train Loss is: 0.0007635916117578745\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.04115567356348038\n",
      "Train Loss is: 0.0009928690269589424\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.09957341849803925\n",
      "Train Loss is: 0.0004853803548030555\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.018385566771030426\n",
      "Train Loss is: 0.0005890838801860809\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.019391726702451706\n",
      "Train Loss is: 0.0007749891374260187\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.00031839084113016725\n",
      "Train Loss is: 0.0005945952143520117\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.010731926187872887\n",
      "Train Loss is: 0.0007590472814626992\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.0007423448842018843\n",
      "Train Loss is: 0.0008878380176611245\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.06445620954036713\n",
      "Train Loss is: 0.0011629818473011255\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.013791303150355816\n",
      "Train Loss is: 0.0018780740210786462\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.07098139822483063\n",
      "Train Loss is: 0.0013446647208184004\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.002576452447101474\n",
      "Train Loss is: 0.0006638171034865081\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.0017836657352745533\n",
      "Train Loss is: 0.0007542187231592834\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.002250968012958765\n",
      "Train Loss is: 0.000779966008849442\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.004239256959408522\n",
      "Train Loss is: 0.000718666531611234\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.0005013223271816969\n",
      "Train Loss is: 0.0006479715812020004\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.008989851921796799\n",
      "Train Loss is: 0.00021129210654180497\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.15621446073055267\n",
      "Train Loss is: 0.0009974342538043857\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.126026451587677\n",
      "Train Loss is: 0.0004208312602713704\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.7714138031005859\n",
      "Train Loss is: 0.00041411948041059077\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.008812110871076584\n",
      "Train Loss is: 0.0006460447330027819\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.008795075118541718\n",
      "Train Loss is: 0.0009447461343370378\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.18569955229759216\n",
      "Train Loss is: 0.0014554680092260242\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.018888210877776146\n",
      "Train Loss is: 0.0003709840530063957\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.005602763965725899\n",
      "Train Loss is: 0.0006672482122667134\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.0008299504988826811\n",
      "Train Loss is: 0.000646079599391669\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.004294956102967262\n",
      "Train Loss is: 0.000558412866666913\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.01451706700026989\n",
      "Train Loss is: 0.000710324093233794\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.03446848317980766\n",
      "Train Loss is: 0.0005477776867337525\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.0252505075186491\n",
      "Train Loss is: 0.0005375000182539225\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.3493024706840515\n",
      "Train Loss is: 0.0004686335159931332\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.01432065200060606\n",
      "Train Loss is: 0.0004904979723505676\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.011178327724337578\n",
      "Train Loss is: 0.0009516915888525546\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.03598177060484886\n",
      "Train Loss is: 0.002466460457071662\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.006395719945430756\n",
      "Train Loss is: 0.000867399328853935\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.015002083964645863\n",
      "Train Loss is: 0.0008552298531867564\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.04441753029823303\n",
      "Train Loss is: 0.0003774769720621407\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.6839102506637573\n",
      "Train Loss is: 0.00045649652020074427\n",
      "Epoch number: 600\n",
      "Test Loss is: 3.581563949584961\n",
      "Train Loss is: 0.00032407158869318664\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.004241457674652338\n",
      "Train Loss is: 0.0004503377713263035\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.05150073394179344\n",
      "Train Loss is: 0.0003148446267005056\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.37529778480529785\n",
      "Train Loss is: 0.0003035333356820047\n",
      "Epoch number: 600\n",
      "Test Loss is: 0.4741556942462921\n",
      "Train Loss is: 0.0007985240081325173\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.008196044713258743\n",
      "Train Loss is: 0.0005643743206746876\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.0020837404299527407\n",
      "Train Loss is: 0.0005376401240937412\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.007175726816058159\n",
      "Train Loss is: 0.0009538316517136991\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.2609553635120392\n",
      "Train Loss is: 0.0006843816954642534\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.008154676295816898\n",
      "Train Loss is: 0.0011383199598640203\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.6150210499763489\n",
      "Train Loss is: 0.0009583658538758755\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.008657068945467472\n",
      "Train Loss is: 0.0012284066760912538\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.005013229791074991\n",
      "Train Loss is: 0.0009520840831100941\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.7047783732414246\n",
      "Train Loss is: 0.00120020960457623\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.003530319547280669\n",
      "Train Loss is: 0.0012156220618635416\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.59794682264328\n",
      "Train Loss is: 0.0005992886144667864\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.004379404243081808\n",
      "Train Loss is: 0.0011824037646874785\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.2636735439300537\n",
      "Train Loss is: 0.000568403338547796\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.10745326429605484\n",
      "Train Loss is: 0.00046383857261389494\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.13052377104759216\n",
      "Train Loss is: 0.0009441671427339315\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.02843344770371914\n",
      "Train Loss is: 0.0011101147392764688\n",
      "Epoch number: 700\n",
      "Test Loss is: 55.71906280517578\n",
      "Train Loss is: 0.0015961752505972981\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.003762546693906188\n",
      "Train Loss is: 0.0005362098454497755\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.02292286790907383\n",
      "Train Loss is: 0.0011341087520122528\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.15001317858695984\n",
      "Train Loss is: 0.0012804233701899648\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.027117889374494553\n",
      "Train Loss is: 0.0008496905211359262\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.028133727610111237\n",
      "Train Loss is: 0.0012553584529086947\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.010590774938464165\n",
      "Train Loss is: 0.0011684333439916372\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.34598156809806824\n",
      "Train Loss is: 0.0011664715129882097\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.03277873620390892\n",
      "Train Loss is: 0.0012620383640751243\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.006183812394738197\n",
      "Train Loss is: 0.0011321555357426405\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.3661492168903351\n",
      "Train Loss is: 0.00041284377221018076\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.10567546635866165\n",
      "Train Loss is: 0.0004385417851153761\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.0011856007622554898\n",
      "Train Loss is: 0.0006875271210446954\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.04542487487196922\n",
      "Train Loss is: 0.001025856239721179\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.10184138268232346\n",
      "Train Loss is: 0.00047166255535557866\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.016719423234462738\n",
      "Train Loss is: 0.0007500352803617716\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.017221195623278618\n",
      "Train Loss is: 0.0007474278099834919\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.0001925679825944826\n",
      "Train Loss is: 0.0004726851766463369\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.007766488008201122\n",
      "Train Loss is: 0.0008161005098372698\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.0009805122390389442\n",
      "Train Loss is: 0.0010233271168544888\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.050786737352609634\n",
      "Train Loss is: 0.0010591994505375624\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.014175619930028915\n",
      "Train Loss is: 0.0017375153256580234\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.07036975771188736\n",
      "Train Loss is: 0.0014161797007545829\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.0020639195572584867\n",
      "Train Loss is: 0.0007348582148551941\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.0013559128856286407\n",
      "Train Loss is: 0.000689111475367099\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.001834980444982648\n",
      "Train Loss is: 0.0006811157218180597\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.0024750176817178726\n",
      "Train Loss is: 0.000813853635918349\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.0013904629740864038\n",
      "Train Loss is: 0.0005167154013179243\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.009645937941968441\n",
      "Train Loss is: 0.0001859035692177713\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.14751286804676056\n",
      "Train Loss is: 0.0009564521024003625\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.1301814466714859\n",
      "Train Loss is: 0.0006700392113998532\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.8037024736404419\n",
      "Train Loss is: 0.0006058827857486904\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.008352489210665226\n",
      "Train Loss is: 0.0005992125370539725\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.00927975494414568\n",
      "Train Loss is: 0.0008879655506461859\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.19172045588493347\n",
      "Train Loss is: 0.0014845163095742464\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.016263896599411964\n",
      "Train Loss is: 0.00041873808368109167\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.004226602613925934\n",
      "Train Loss is: 0.0007699063862673938\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.0007164296112023294\n",
      "Train Loss is: 0.0007397994049824774\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.0009674323373474181\n",
      "Train Loss is: 0.0005881162942387164\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.017921945080161095\n",
      "Train Loss is: 0.0007163721602410078\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.028399638831615448\n",
      "Train Loss is: 0.0006342155393213034\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.026653720065951347\n",
      "Train Loss is: 0.0006408128538168967\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.3437674939632416\n",
      "Train Loss is: 0.0006060860469006002\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.017958201467990875\n",
      "Train Loss is: 0.0006266215350478888\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.012069777585566044\n",
      "Train Loss is: 0.0008826754055917263\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.032830070704221725\n",
      "Train Loss is: 0.0023392164148390293\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.007755061145871878\n",
      "Train Loss is: 0.0009530132520012558\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.014055765233933926\n",
      "Train Loss is: 0.0008746277308091521\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.05297308787703514\n",
      "Train Loss is: 0.0005423273541964591\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.7331257462501526\n",
      "Train Loss is: 0.0005054002976976335\n",
      "Epoch number: 700\n",
      "Test Loss is: 3.6548030376434326\n",
      "Train Loss is: 0.0004907335387542844\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.0038274319376796484\n",
      "Train Loss is: 0.0004149938467890024\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.07141751796007156\n",
      "Train Loss is: 0.00029070209711790085\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.41114115715026855\n",
      "Train Loss is: 0.00029362659552134573\n",
      "Epoch number: 700\n",
      "Test Loss is: 0.48671984672546387\n",
      "Train Loss is: 0.0005638966686092317\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.007695477455854416\n",
      "Train Loss is: 0.0005166562041267753\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.0028576459735631943\n",
      "Train Loss is: 0.0007027196697890759\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.006776872556656599\n",
      "Train Loss is: 0.0007025511586107314\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.2733074724674225\n",
      "Train Loss is: 0.0006562941707670689\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.00805178564041853\n",
      "Train Loss is: 0.001066066324710846\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.7145395874977112\n",
      "Train Loss is: 0.0009985662763938308\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.010957593098282814\n",
      "Train Loss is: 0.0013434103457257152\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.012573723681271076\n",
      "Train Loss is: 0.000629480229690671\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.8212547302246094\n",
      "Train Loss is: 0.00044734831317327917\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.0033208841923624277\n",
      "Train Loss is: 0.0008764527156017721\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.6982598304748535\n",
      "Train Loss is: 0.0005574926035478711\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.002588696777820587\n",
      "Train Loss is: 0.0009786636801436543\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.295065701007843\n",
      "Train Loss is: 0.0006101790932007134\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.1264895796775818\n",
      "Train Loss is: 0.000758002104703337\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.14466695487499237\n",
      "Train Loss is: 0.0009593436261638999\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.02644406259059906\n",
      "Train Loss is: 0.0010059337364509702\n",
      "Epoch number: 800\n",
      "Test Loss is: 55.96445846557617\n",
      "Train Loss is: 0.001744379522278905\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.003998204134404659\n",
      "Train Loss is: 0.0004919555503875017\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.0209913682192564\n",
      "Train Loss is: 0.0007747283088974655\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.14178520441055298\n",
      "Train Loss is: 0.001346209435723722\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.033370327204465866\n",
      "Train Loss is: 0.0009833336807787418\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.029673460870981216\n",
      "Train Loss is: 0.0007018691976554692\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.01125295553356409\n",
      "Train Loss is: 0.001193741336464882\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.33678290247917175\n",
      "Train Loss is: 0.001061714836396277\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.028175145387649536\n",
      "Train Loss is: 0.0011232745600864291\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.007544091437011957\n",
      "Train Loss is: 0.0010005265939980745\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.4327428638935089\n",
      "Train Loss is: 0.0003941205213777721\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.13318252563476562\n",
      "Train Loss is: 0.00033413886558264494\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.0009387845639139414\n",
      "Train Loss is: 0.0004456500173546374\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.03361552208662033\n",
      "Train Loss is: 0.0008841530070640147\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.09762100130319595\n",
      "Train Loss is: 0.0004227341851219535\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.015812210738658905\n",
      "Train Loss is: 0.0005482812412083149\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.01674695499241352\n",
      "Train Loss is: 0.00063608173513785\n",
      "Epoch number: 800\n",
      "Test Loss is: 9.258301724912599e-05\n",
      "Train Loss is: 0.00025305396411567926\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.006573762744665146\n",
      "Train Loss is: 0.00068584707332775\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.0012392759090289474\n",
      "Train Loss is: 0.0009262511157430708\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.044822413474321365\n",
      "Train Loss is: 0.0008885489078238606\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.013697812333703041\n",
      "Train Loss is: 0.0016742118168622255\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.06297201663255692\n",
      "Train Loss is: 0.0013971333391964436\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.0027951558586210012\n",
      "Train Loss is: 0.0005761615466326475\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.0010347249917685986\n",
      "Train Loss is: 0.0006619911873713136\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.0006574533763341606\n",
      "Train Loss is: 0.0006621922948397696\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.002417011884972453\n",
      "Train Loss is: 0.0005635909037664533\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.0015239849453791976\n",
      "Train Loss is: 0.00047002421342767775\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.009335187263786793\n",
      "Train Loss is: 0.00017270108219236135\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.2083980143070221\n",
      "Train Loss is: 0.000871251686476171\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.1504548043012619\n",
      "Train Loss is: 0.00035478794598020613\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.8313605189323425\n",
      "Train Loss is: 0.0003809796762652695\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.007931819185614586\n",
      "Train Loss is: 0.000528513512108475\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.007587427739053965\n",
      "Train Loss is: 0.0008537735557183623\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.21719977259635925\n",
      "Train Loss is: 0.001363923423923552\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.017576949670910835\n",
      "Train Loss is: 0.0003646841796580702\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.003626135876402259\n",
      "Train Loss is: 0.0006531930412165821\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.0012329855235293508\n",
      "Train Loss is: 0.0005658259615302086\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.0009795865043997765\n",
      "Train Loss is: 0.00046915069106034935\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.01608903892338276\n",
      "Train Loss is: 0.0006723461556248367\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.03488030657172203\n",
      "Train Loss is: 0.0004885858506895602\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.02448115311563015\n",
      "Train Loss is: 0.0004917298210784793\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.3245829641819\n",
      "Train Loss is: 0.0005111631471663713\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.019277770072221756\n",
      "Train Loss is: 0.0004349996452219784\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.014762955717742443\n",
      "Train Loss is: 0.0008977145771495998\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.028086556121706963\n",
      "Train Loss is: 0.0022533838637173176\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.007520666345953941\n",
      "Train Loss is: 0.0007815008866600692\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.017636986449360847\n",
      "Train Loss is: 0.0006287713767960668\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.03742728754878044\n",
      "Train Loss is: 0.0001868936378741637\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.6845457553863525\n",
      "Train Loss is: 0.00034122387296520174\n",
      "Epoch number: 800\n",
      "Test Loss is: 3.535655975341797\n",
      "Train Loss is: 0.0002322495711268857\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.005342262797057629\n",
      "Train Loss is: 0.0003864700556732714\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.06334490329027176\n",
      "Train Loss is: 0.0002844918635673821\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.3849973976612091\n",
      "Train Loss is: 0.00025908651878125966\n",
      "Epoch number: 800\n",
      "Test Loss is: 0.4808284640312195\n",
      "Train Loss is: 0.0005397834465838969\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.009583829902112484\n",
      "Train Loss is: 0.0004386136424727738\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.0018275664187967777\n",
      "Train Loss is: 0.00054241216275841\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.006997486110776663\n",
      "Train Loss is: 0.0007355842972174287\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.3109051585197449\n",
      "Train Loss is: 0.0006174211157485843\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.008511468768119812\n",
      "Train Loss is: 0.0009630079148337245\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.7399506568908691\n",
      "Train Loss is: 0.0009515709825791419\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.007756386883556843\n",
      "Train Loss is: 0.0011906480649486184\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.009683379903435707\n",
      "Train Loss is: 0.0006690190639346838\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.7354893684387207\n",
      "Train Loss is: 0.0004512254672590643\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.0032452938612550497\n",
      "Train Loss is: 0.0010141251841560006\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.6107372641563416\n",
      "Train Loss is: 0.0005162493325769901\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.003290202235803008\n",
      "Train Loss is: 0.0008734947186894715\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.27502143383026123\n",
      "Train Loss is: 0.0005720231565646827\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.10463177412748337\n",
      "Train Loss is: 0.0005200363812036812\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.11741318553686142\n",
      "Train Loss is: 0.0010498546762391925\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.029790036380290985\n",
      "Train Loss is: 0.0012199823977425694\n",
      "Epoch number: 900\n",
      "Test Loss is: 55.98678970336914\n",
      "Train Loss is: 0.002344816457480192\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.0036504026502370834\n",
      "Train Loss is: 0.000531403289642185\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.015870055183768272\n",
      "Train Loss is: 0.0007866588421165943\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.11717355251312256\n",
      "Train Loss is: 0.0014054772909730673\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.04505099356174469\n",
      "Train Loss is: 0.0008712923736311495\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.025771114975214005\n",
      "Train Loss is: 0.0010568565921857953\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.019418252632021904\n",
      "Train Loss is: 0.0013924670638516545\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.31202250719070435\n",
      "Train Loss is: 0.0013733415398746729\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.02829163521528244\n",
      "Train Loss is: 0.001491947565227747\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.0073206922970712185\n",
      "Train Loss is: 0.0010259379632771015\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.3399655520915985\n",
      "Train Loss is: 0.00035442327498458326\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.10201037675142288\n",
      "Train Loss is: 0.00042024196591228247\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.0015693202149122953\n",
      "Train Loss is: 0.000655889161862433\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.031038129702210426\n",
      "Train Loss is: 0.0008746729581616819\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.11775730550289154\n",
      "Train Loss is: 0.0004317605053074658\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.027335869148373604\n",
      "Train Loss is: 0.0007465786184184253\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.01718788966536522\n",
      "Train Loss is: 0.0007188970921561122\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.00019914889708161354\n",
      "Train Loss is: 0.00028356772963888943\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.005740814376622438\n",
      "Train Loss is: 0.000689466658513993\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.003360125469043851\n",
      "Train Loss is: 0.0008040840621106327\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.04256885126233101\n",
      "Train Loss is: 0.0011581876315176487\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.015712721273303032\n",
      "Train Loss is: 0.001778714475221932\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.07015928626060486\n",
      "Train Loss is: 0.0013216017978265882\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.001957444241270423\n",
      "Train Loss is: 0.0006495351553894579\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.001274656504392624\n",
      "Train Loss is: 0.0006499018636532128\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.0007401609909720719\n",
      "Train Loss is: 0.000770792830735445\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.0029525712598115206\n",
      "Train Loss is: 0.0005667382501997054\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.001681803958490491\n",
      "Train Loss is: 0.0005074067739769816\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.011815383099019527\n",
      "Train Loss is: 0.0003240406222175807\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.20002633333206177\n",
      "Train Loss is: 0.0009081828757189214\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.14658434689044952\n",
      "Train Loss is: 0.00034853327088057995\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.745480477809906\n",
      "Train Loss is: 0.00038481716183014214\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.007896422408521175\n",
      "Train Loss is: 0.0005231339018791914\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.007345810066908598\n",
      "Train Loss is: 0.0008805319084785879\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.17345592379570007\n",
      "Train Loss is: 0.0015036568511277437\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.021590938791632652\n",
      "Train Loss is: 0.0003994037979282439\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.007628403138369322\n",
      "Train Loss is: 0.0007365989149548113\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.0008734036237001419\n",
      "Train Loss is: 0.0005371419829316437\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.0020652671810239553\n",
      "Train Loss is: 0.00048418843653053045\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.014162628911435604\n",
      "Train Loss is: 0.0007598113152198493\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.04264349490404129\n",
      "Train Loss is: 0.0005765367532148957\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.020022079348564148\n",
      "Train Loss is: 0.0005193689721636474\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.3136341869831085\n",
      "Train Loss is: 0.00040703272679820657\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.01776360347867012\n",
      "Train Loss is: 0.00045108378981240094\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.013653736561536789\n",
      "Train Loss is: 0.0008422142127528787\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.03087647259235382\n",
      "Train Loss is: 0.002180545823648572\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.006667542736977339\n",
      "Train Loss is: 0.0007877452298998833\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.024769453331828117\n",
      "Train Loss is: 0.000983032863587141\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.028659241273999214\n",
      "Train Loss is: 0.00036865341826342046\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.6768038868904114\n",
      "Train Loss is: 0.0005463833804242313\n",
      "Epoch number: 900\n",
      "Test Loss is: 3.600980043411255\n",
      "Train Loss is: 0.00037793361116200686\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.004427641164511442\n",
      "Train Loss is: 0.0003764233260881156\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.06999745219945908\n",
      "Train Loss is: 0.0002564328024163842\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.43255871534347534\n",
      "Train Loss is: 0.00020461552776396275\n",
      "Epoch number: 900\n",
      "Test Loss is: 0.4986131191253662\n",
      "Train Loss is: 0.0004848231910727918\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.007591794710606337\n",
      "Train Loss is: 0.0003740878601092845\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.0019734343513846397\n",
      "Train Loss is: 0.00041201160638593137\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.005220677703619003\n",
      "Train Loss is: 0.0005194516852498055\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.2910444736480713\n",
      "Train Loss is: 0.00039720855420455337\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.007806698326021433\n",
      "Train Loss is: 0.0008163962047547102\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.6501960754394531\n",
      "Train Loss is: 0.0007402377086691558\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.008096983656287193\n",
      "Train Loss is: 0.001007684739306569\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.01259272638708353\n",
      "Train Loss is: 0.0004770300292875618\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.7204678654670715\n",
      "Train Loss is: 0.0003378540277481079\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.004942953586578369\n",
      "Train Loss is: 0.00073769356822595\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.620916485786438\n",
      "Train Loss is: 0.0003944664786104113\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.0017616520635783672\n",
      "Train Loss is: 0.000758903450332582\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.26457810401916504\n",
      "Train Loss is: 0.0004853536083828658\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.10513550043106079\n",
      "Train Loss is: 0.00042326602851971984\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.10933474451303482\n",
      "Train Loss is: 0.0008135535754263401\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.023242399096488953\n",
      "Train Loss is: 0.0008297936292365193\n",
      "Epoch number: 999\n",
      "Test Loss is: 55.90933609008789\n",
      "Train Loss is: 0.0011284243082627654\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.004848364740610123\n",
      "Train Loss is: 0.0003578642208594829\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.019286874681711197\n",
      "Train Loss is: 0.0005986955948174\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.13323171436786652\n",
      "Train Loss is: 0.001166607951745391\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.03302093595266342\n",
      "Train Loss is: 0.0007585811545141041\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.03167002275586128\n",
      "Train Loss is: 0.0007286198087967932\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.011509780772030354\n",
      "Train Loss is: 0.0010129166767001152\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.34045103192329407\n",
      "Train Loss is: 0.0010581848910078406\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.03061402030289173\n",
      "Train Loss is: 0.0009494785335846245\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.0034087728708982468\n",
      "Train Loss is: 0.0008186015184037387\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.38067084550857544\n",
      "Train Loss is: 0.00024026876781135798\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.11524822562932968\n",
      "Train Loss is: 0.0003603310324251652\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.0001871868735179305\n",
      "Train Loss is: 0.0003546550578903407\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.044503793120384216\n",
      "Train Loss is: 0.0007851494592614472\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.13471759855747223\n",
      "Train Loss is: 0.000372648355551064\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.02607937529683113\n",
      "Train Loss is: 0.0005196978454478085\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.019656823948025703\n",
      "Train Loss is: 0.0005332370637916028\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.0002849627926480025\n",
      "Train Loss is: 0.0002165408368455246\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.006718111224472523\n",
      "Train Loss is: 0.0005507542518898845\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.0021222999785095453\n",
      "Train Loss is: 0.0007382023031823337\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.05094292759895325\n",
      "Train Loss is: 0.0006852783262729645\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.01743796281516552\n",
      "Train Loss is: 0.0013870717957615852\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.0764395222067833\n",
      "Train Loss is: 0.0010272389044985175\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.002902429550886154\n",
      "Train Loss is: 0.0005185859627090394\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.0012281753588467836\n",
      "Train Loss is: 0.0005579930148087442\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.0022103500086814165\n",
      "Train Loss is: 0.0005629371153190732\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.003183879656717181\n",
      "Train Loss is: 0.0005371214938350022\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.0009456676780246198\n",
      "Train Loss is: 0.00045182512258179486\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.010960789397358894\n",
      "Train Loss is: 0.00017639872385188937\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.17813299596309662\n",
      "Train Loss is: 0.0007148263393901289\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.11631123721599579\n",
      "Train Loss is: 0.0003462368331383914\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.6532760858535767\n",
      "Train Loss is: 0.00040224145050160587\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.009515917859971523\n",
      "Train Loss is: 0.00041943800169974566\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.005821294616907835\n",
      "Train Loss is: 0.0007125043775886297\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.16701479256153107\n",
      "Train Loss is: 0.0011908907908946276\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.021609261631965637\n",
      "Train Loss is: 0.00032225309405475855\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.005987902637571096\n",
      "Train Loss is: 0.0005631471867673099\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.0006945336936041713\n",
      "Train Loss is: 0.0004100048681721091\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.0013406750513240695\n",
      "Train Loss is: 0.00045722146751359105\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.013652335852384567\n",
      "Train Loss is: 0.0006966274813748896\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.03486322984099388\n",
      "Train Loss is: 0.0004310663207434118\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.02697802521288395\n",
      "Train Loss is: 0.0004784913326147944\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.36211422085762024\n",
      "Train Loss is: 0.0003923701588064432\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.020024195313453674\n",
      "Train Loss is: 0.0004259984416421503\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.011319175362586975\n",
      "Train Loss is: 0.0006908886134624481\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.018620846793055534\n",
      "Train Loss is: 0.0018962512258440256\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.007728256285190582\n",
      "Train Loss is: 0.0006328102317638695\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.01503542996942997\n",
      "Train Loss is: 0.0006200846983119845\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.04422103986144066\n",
      "Train Loss is: 0.00017069563909899443\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.7134908437728882\n",
      "Train Loss is: 0.0004145702696405351\n",
      "Epoch number: 999\n",
      "Test Loss is: 3.6947858333587646\n",
      "Train Loss is: 0.000218852364923805\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.005041264928877354\n",
      "Train Loss is: 0.00032798005850054324\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.06363357603549957\n",
      "Train Loss is: 0.00024645315716043115\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.40085113048553467\n",
      "Train Loss is: 0.00023307133233174682\n",
      "Epoch number: 999\n",
      "Test Loss is: 0.4862633943557739\n",
      "Train Loss is: 0.0004694718518294394\n"
     ]
    }
   ],
   "source": [
    "test = StocksDataSet(\"../Custom LSTM Model/Data/Stocks Data\")\n",
    "our_model = train_model(test, epochs_number=1000, learning_rate=0.001, hidden_size = 256, num_classes=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac019438-82e1-4063-b62c-2fc3d6bea074",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(our_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45226b8d-7933-461c-b5bf-90845c4a17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(our_model, model_name=\"kwargs_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3785f3d8-39d0-41bd-a31c-42861968f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_test = load_model(model_name=\"kwargs_test\")\n",
    "save_with_kwargs(load_test, name = \"Saved_with_kwargs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cc83e8ce-dd18-4df7-825d-24961ec71565",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs_model = load_with_kwargs(\"Saved_with_kwargs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d8bccc90-9c29-48fd-9574-ab712334d8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3922, 0.3872, 0.3795, 0.3691, 0.3597, 0.3518, 0.3429, 0.3382, 0.3332,\n",
      "         0.3261, 0.3185, 0.3091, 0.2992, 0.2918, 0.2847, 0.2757, 0.2683, 0.2619,\n",
      "         0.2590, 0.2539, 0.2520, 0.2493, 0.2488, 0.2486, 0.2486, 0.2501, 0.2505,\n",
      "         0.2463, 0.2449, 0.2440]])\n",
      "[[174.11223638 176.4672007  179.0494729  171.73914403 172.63968042\n",
      "  171.20178223 170.05295017 173.6481843  171.81317002 170.35613972\n",
      "  169.15846413 169.95914155 169.11922505 169.87177321 168.734658\n",
      "  169.94567537 168.04826319 175.30188262 176.80903631 172.94394839\n",
      "  169.63199851 168.2493203  167.28879771 165.24858908 166.0885582\n",
      "  167.15006965 169.27045494 170.13631214 169.54485804 173.74401052]]\n"
     ]
    }
   ],
   "source": [
    "single_stock = pd.read_csv(\"./Data/Stocks Data/Technology/Apple Inc. Common Stock.csv\", index_col=\"Date\", parse_dates = True)\n",
    "preds = predict_stock_price(single_stock, kwargs_model)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84672f76-02d9-4c9d-b04f-efc152f6bab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5825, 0.5889, 0.5944, 0.6020, 0.6083, 0.6148, 0.6165, 0.6202, 0.6232,\n",
      "         0.6231, 0.6225, 0.6197, 0.6143, 0.6127, 0.6090, 0.6007, 0.5911, 0.5820,\n",
      "         0.5785, 0.5715, 0.5649, 0.5578, 0.5524, 0.5479, 0.5426, 0.5386, 0.5354,\n",
      "         0.5300, 0.5269, 0.5236]])\n",
      "[[162.44250405 162.37891901 165.30438805 168.78203076 168.46833658\n",
      "  168.60483003 168.00649929 167.03023565 165.58322418 163.9930582\n",
      "  160.73245978 159.95967788 159.45429254 162.38266754 160.8790189\n",
      "  163.26067722 160.87108964 166.00204355 159.94846535 156.17146623\n",
      "  157.22488654 158.92783141 160.65242863 155.55792946 157.15256059\n",
      "  161.48860222 163.37538958 163.53999597 164.75686371 162.7736007 ]]\n"
     ]
    }
   ],
   "source": [
    "single_stock = pd.read_csv(\"./Data/Stocks Data/Consumer Discretionary/Airbnb Inc. Class A Common Stock.csv\", index_col=\"Date\", parse_dates = True)\n",
    "preds = predict_stock_price(single_stock, kwargs_model)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f863d9-d462-4adc-ab66-179da5b22002",
   "metadata": {},
   "source": [
    "# Multithreading training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833ba6f-a847-4fd4-b236-06c6e4d62989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model):\n",
    "#     # Construct data_loader, optimizer, etc.\n",
    "#     for data, labels in data_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         loss_fn(model(data), labels).backward()\n",
    "#         optimizer.step()  # This will update the shared parameters\n",
    "\n",
    "def train(model):\n",
    "    custom_ds = StocksDataSet(\"../Custom LSTM Model/Data/Stocks Data\")\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=learning_rate)\n",
    "    epochs = 100\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for X_train, X_test, y_train, y_test in custom_ds:\n",
    "            X_train = torch.Tensor(X_train).squeeze(0)\n",
    "            model.train()\n",
    "            model = model.to(device)\n",
    "            X_train.to(device)\n",
    "       \n",
    "            y_pred = model(X_train)\n",
    "         \n",
    "          \n",
    "            optimizer.zero_grad()\n",
    "            y_pred = y_pred.to(device)\n",
    "            y_train = y_train.to(device)\n",
    "            loss = loss_fn(y_pred, y_train)\n",
    "            loss = loss.to(device)\n",
    "            # torch.cuda.empty_cache()\n",
    "            # gc.collect()\n",
    "            loss.backward()\n",
    "        \n",
    "            optimizer.step()\n",
    "        \n",
    "            model.eval()\n",
    "            \n",
    "        \n",
    "            with torch.inference_mode():\n",
    "            \n",
    "                test_pred = model(X_test)\n",
    "                test_pred.to(device)\n",
    "                y_test = y_test.to(device)\n",
    "                test_loss = loss_fn(test_pred, y_test)\n",
    "                if epoch == epochs - 1 or epoch % 100 == 0:\n",
    "                    print(f\"Epoch number: {epoch}\")\n",
    "                    print(f\"Test Loss is: {test_loss}\")\n",
    "                    print(f\"Train Loss is: {loss}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    #return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f96321-ec37-4f0a-9df5-a493b219600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    num_processes = 4\n",
    "    model = StocksPredictionModel(input_size=16, hidden_size=256, num_layers=1, num_classes = 30, batch_first=True)\n",
    "        # NOTE: this is required for the ``fork`` method to work\n",
    "    model.share_memory()\n",
    "    processes = []\n",
    "    for rank in range(num_processes):\n",
    "        p = mp.Process(target=train, args=(model,))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "    save_model(model, model_name = \"Multithreading model test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
